{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from torch.optim import lr_scheduler\n",
    "from time import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\",\n",
    "                         dtype={\"ID_code\": np.str, \"target\": np.bool})\n",
    "\n",
    "test_data = pd.read_csv(\"test.csv\",\n",
    "                         dtype={\"ID_code\": np.str, \"target\": np.bool})\n",
    "\n",
    "test_data = test_data.drop(['ID_code'], axis = 1)\n",
    "test_data = ((test_data-test_data.min())/(test_data.max()-test_data.min()))\n",
    "#############TRAIN ON PART (SANITY TEST)\n",
    "#train_data = train_data.head(2000)\n",
    "\n",
    "train_y = train_data.target\n",
    "train_x = train_data.drop(['target', 'ID_code'], axis=1)\n",
    "train_x = ((train_x-train_x.min())/(train_x.max()-train_x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, X_test, y_train, y_test = train_test_split(\n",
    "     train_x, train_y, test_size=0.25, random_state=333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "hidden_size = 512\n",
    "\n",
    "tensor_x = torch.tensor(X_train.values).float()\n",
    "tensor_y = torch.tensor(y_train.values.astype(np.int32)).float().view(-1,1)\n",
    "tensor_test_x = torch.tensor(X_test.values).float()\n",
    "tensor_test_y = torch.tensor(y_test.values.astype(np.int32)).float().view(-1,1)\n",
    "\n",
    "train_dataset = TensorDataset(tensor_x, tensor_y)\n",
    "dataloader = DataLoader(train_dataset, batch_size = batch_size,\n",
    "                                         num_workers = 6, shuffle = True)\n",
    "\n",
    "test_dataset = TensorDataset(tensor_test_x, tensor_test_y)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size = batch_size,\n",
    "                                         num_workers = 6, shuffle = False)\n",
    "#print(next(iter(dataloader))[0][0])\n",
    "#print(next(iter(dataloader))[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lin1): Linear(in_features=200, out_features=512, bias=True)\n",
      "  (lin2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (lin3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (dropout): AlphaDropout(p=0.5)\n",
      "  (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_dim, batch_size, output_dim=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.lin1 = nn.Linear(input_size, hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin3 = nn.Linear(hidden_dim, output_dim)\n",
    "        #self.selu = nn.functional.selu()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        #Alpha Dropout is a Dropout that keeps mean and variance of inputs to their original values,\n",
    "        #in order to ensure the self-normalizing property even after this dropout.\n",
    "        self.dropout = nn.AlphaDropout(p=0.5)\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_dim).cuda()\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Forward pass through FFN\n",
    "        layer1 = self.lin1(input)\n",
    "        #layer1 = self.batchnorm(layer1)\n",
    "        layer1 = self.dropout(layer1)\n",
    "        ##########Scaled Exponential Linear Unit - https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9\n",
    "        layer1 = nn.functional.selu(layer1)\n",
    "        layer2 = self.lin2(layer1)\n",
    "        #layer2 = self.batchnorm(layer2)\n",
    "        layer2 = self.dropout(layer2)\n",
    "        layer2 = nn.functional.selu(layer2)\n",
    "        layer3 = self.lin2(layer2)\n",
    "        #layer3 = self.batchnorm(layer3)\n",
    "        layer3 = self.dropout(layer3)\n",
    "        layer3 = nn.functional.selu(layer3)\n",
    "        layer4 = self.lin3(layer3)\n",
    "        y_pred = self.sigmoid(layer4)\n",
    "        return y_pred\n",
    "\n",
    "model = Net(input_size=200, hidden_dim=hidden_size, batch_size=batch_size).cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8876dd7467054873bbb5b2040d900395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 BCE:  0.29924333095550537 ROC train:  0.5006052256790489 ROC val:  0.49822156952193875\n",
      "Epoch:  1 BCE:  0.33446618914604187 ROC train:  0.500821399708918 ROC val:  0.5133679300631236\n",
      "Epoch:  2 BCE:  0.3692208230495453 ROC train:  0.5040171739430327 ROC val:  0.5012719654773964\n",
      "Epoch:  3 BCE:  0.30202415585517883 ROC train:  0.5024577985481953 ROC val:  0.5089297232638252\n",
      "Epoch:  4 BCE:  0.32144346833229065 ROC train:  0.5081732096384983 ROC val:  0.503780473664202\n",
      "Epoch:  5 BCE:  0.39583101868629456 ROC train:  0.5092143701713665 ROC val:  0.5126921223751127\n",
      "Epoch:  6 BCE:  0.2724354565143585 ROC train:  0.5148907669109902 ROC val:  0.5163690476387592\n",
      "Epoch:  7 BCE:  0.2895214855670929 ROC train:  0.5224874228348392 ROC val:  0.511040829719604\n",
      "Epoch:  8 BCE:  0.32779550552368164 ROC train:  0.5234688133893552 ROC val:  0.5202811628616741\n",
      "Epoch:  9 BCE:  0.29498323798179626 ROC train:  0.5237258425266985 ROC val:  0.5246725440511869\n",
      "Epoch:  10 BCE:  0.34653982520103455 ROC train:  0.5288132942717033 ROC val:  0.5271289833079219\n",
      "Epoch:  11 BCE:  0.2785141170024872 ROC train:  0.531366617567698 ROC val:  0.5308176337395822\n",
      "Epoch:  12 BCE:  0.3244627118110657 ROC train:  0.5364505646908712 ROC val:  0.5413134782680666\n",
      "Epoch:  13 BCE:  0.29930782318115234 ROC train:  0.5421888154840686 ROC val:  0.5393978008459438\n",
      "Epoch:  14 BCE:  0.3748307526111603 ROC train:  0.5446063732484623 ROC val:  0.5461429979237202\n",
      "Epoch:  15 BCE:  0.3197084367275238 ROC train:  0.5495379171737669 ROC val:  0.5515747377508653\n",
      "Epoch:  16 BCE:  0.3666194677352905 ROC train:  0.5562593308834092 ROC val:  0.5498006454304292\n",
      "Epoch:  17 BCE:  0.3227725923061371 ROC train:  0.5565111098536843 ROC val:  0.5614674812300504\n",
      "Epoch:  18 BCE:  0.3801717460155487 ROC train:  0.5655859695054877 ROC val:  0.5614882823095129\n",
      "Epoch:  19 BCE:  0.30078020691871643 ROC train:  0.5727915226722444 ROC val:  0.5745953930638162\n",
      "Epoch:  20 BCE:  0.2930527925491333 ROC train:  0.576928598966362 ROC val:  0.5728216252773093\n",
      "Epoch:  21 BCE:  0.30579230189323425 ROC train:  0.5824448540472474 ROC val:  0.5805813374949269\n",
      "Epoch:  22 BCE:  0.3215332329273224 ROC train:  0.5841092930594982 ROC val:  0.5841487998927369\n",
      "Epoch:  23 BCE:  0.2606247663497925 ROC train:  0.588252264351419 ROC val:  0.5928717663433773\n",
      "Epoch:  24 BCE:  0.26853421330451965 ROC train:  0.5969234004251166 ROC val:  0.6090980322995683\n",
      "Epoch:  25 BCE:  0.3100587725639343 ROC train:  0.6065773557187553 ROC val:  0.6079641460743376\n",
      "Epoch:  26 BCE:  0.360954225063324 ROC train:  0.608727116582822 ROC val:  0.6015993888624309\n",
      "Epoch:  27 BCE:  0.27966737747192383 ROC train:  0.6114169274929082 ROC val:  0.6236492726768558\n",
      "Epoch:  28 BCE:  0.293828547000885 ROC train:  0.6182396150555067 ROC val:  0.6225146049209384\n",
      "Epoch:  29 BCE:  0.2571660578250885 ROC train:  0.6241513445063538 ROC val:  0.6238317821693298\n",
      "Epoch:  30 BCE:  0.3480655252933502 ROC train:  0.6253512635471046 ROC val:  0.6247928375194037\n",
      "Epoch:  31 BCE:  0.28618112206459045 ROC train:  0.6321362323250868 ROC val:  0.6314463681123188\n",
      "Epoch:  32 BCE:  0.3833841383457184 ROC train:  0.6348030441082739 ROC val:  0.6385576350468137\n",
      "Epoch:  33 BCE:  0.33134278655052185 ROC train:  0.6397333483151222 ROC val:  0.6415731005544625\n",
      "Epoch:  34 BCE:  0.3700876832008362 ROC train:  0.6451570553010366 ROC val:  0.6483491835483586\n",
      "Epoch:  35 BCE:  0.288191020488739 ROC train:  0.6511913768909982 ROC val:  0.6526082167107412\n",
      "Epoch:  36 BCE:  0.35361799597740173 ROC train:  0.6594949247562466 ROC val:  0.6523493203206443\n",
      "Epoch:  37 BCE:  0.32055458426475525 ROC train:  0.6616073872919068 ROC val:  0.6620042731536999\n",
      "Epoch:  38 BCE:  0.2567169666290283 ROC train:  0.6657386726566688 ROC val:  0.6677219406187598\n",
      "Epoch:  39 BCE:  0.31808680295944214 ROC train:  0.6679811801576383 ROC val:  0.6701845504355773\n",
      "Epoch:  40 BCE:  0.32991164922714233 ROC train:  0.6763442522225647 ROC val:  0.6768335397611971\n",
      "Epoch:  41 BCE:  0.39502981305122375 ROC train:  0.6823048010924289 ROC val:  0.6821105696280899\n",
      "Epoch:  42 BCE:  0.40123066306114197 ROC train:  0.6844686303868355 ROC val:  0.6847990064899323\n",
      "Epoch:  43 BCE:  0.2769809067249298 ROC train:  0.6897664534231681 ROC val:  0.6930640025813296\n",
      "Epoch:  44 BCE:  0.3059822916984558 ROC train:  0.6954176478847168 ROC val:  0.695093639982308\n",
      "Epoch:  45 BCE:  0.29167047142982483 ROC train:  0.7032168456098855 ROC val:  0.7034832747642316\n",
      "Epoch:  46 BCE:  0.2369861602783203 ROC train:  0.7065490038816936 ROC val:  0.7071308793808455\n",
      "Epoch:  47 BCE:  0.36432912945747375 ROC train:  0.7132057780461247 ROC val:  0.7205413962432509\n",
      "Epoch:  48 BCE:  0.31824594736099243 ROC train:  0.7172955479731213 ROC val:  0.7178988069739789\n",
      "Epoch:  49 BCE:  0.2608244717121124 ROC train:  0.7219236351941853 ROC val:  0.7213418492449293\n",
      "Epoch:  50 BCE:  0.3268581032752991 ROC train:  0.729473119818473 ROC val:  0.7289522493625458\n",
      "Epoch:  51 BCE:  0.32719072699546814 ROC train:  0.7312011948411894 ROC val:  0.7315413876729059\n",
      "Epoch:  52 BCE:  0.2684997320175171 ROC train:  0.7375325536129309 ROC val:  0.740454519967493\n",
      "Epoch:  53 BCE:  0.27468210458755493 ROC train:  0.7407428838983384 ROC val:  0.7447407521639426\n",
      "Epoch:  54 BCE:  0.38656914234161377 ROC train:  0.7418617247615915 ROC val:  0.752703770538069\n",
      "Epoch:  55 BCE:  0.295768678188324 ROC train:  0.7505400969137246 ROC val:  0.7583207772922295\n",
      "Epoch:  56 BCE:  0.2547813355922699 ROC train:  0.7525690933561626 ROC val:  0.7618534306664897\n",
      "Epoch:  57 BCE:  0.3429761826992035 ROC train:  0.7560008623539642 ROC val:  0.7601365357952911\n",
      "Epoch:  58 BCE:  0.28427061438560486 ROC train:  0.7610949840745728 ROC val:  0.7628544323901357\n",
      "Epoch:  59 BCE:  0.31081047654151917 ROC train:  0.7636522255599174 ROC val:  0.7657708421948207\n",
      "Epoch:  60 BCE:  0.21423886716365814 ROC train:  0.7683856189787475 ROC val:  0.7699340407086795\n",
      "Epoch:  61 BCE:  0.3196631371974945 ROC train:  0.7680425835431691 ROC val:  0.7768456309681597\n",
      "Epoch:  62 BCE:  0.3127812445163727 ROC train:  0.7733436425423623 ROC val:  0.7772063316649065\n",
      "Epoch:  63 BCE:  0.2913222312927246 ROC train:  0.7760076786748998 ROC val:  0.7787135356712748\n",
      "Epoch:  64 BCE:  0.26802822947502136 ROC train:  0.7789412058338386 ROC val:  0.7828531911251265\n",
      "Epoch:  65 BCE:  0.23970232903957367 ROC train:  0.7785076579116454 ROC val:  0.7796887380514059\n",
      "Epoch:  66 BCE:  0.266558974981308 ROC train:  0.7832752354782472 ROC val:  0.7869952264944591\n",
      "Epoch:  67 BCE:  0.2997652590274811 ROC train:  0.7839979460696924 ROC val:  0.7826134488599942\n",
      "Epoch:  68 BCE:  0.26595890522003174 ROC train:  0.7857010894415023 ROC val:  0.7892300046621176\n",
      "Epoch:  69 BCE:  0.2584637701511383 ROC train:  0.7878233774655505 ROC val:  0.7874273798484768\n",
      "Epoch:  70 BCE:  0.24845270812511444 ROC train:  0.7917389291846537 ROC val:  0.7941709255563276\n",
      "Epoch:  71 BCE:  0.32188013195991516 ROC train:  0.7930786224834447 ROC val:  0.7967137813544979\n",
      "Epoch:  72 BCE:  0.3132696747779846 ROC train:  0.793170471622247 ROC val:  0.7959387656312705\n",
      "Epoch:  73 BCE:  0.28994137048721313 ROC train:  0.796033144652088 ROC val:  0.797191161933507\n",
      "Epoch:  74 BCE:  0.21194122731685638 ROC train:  0.7948251687945073 ROC val:  0.7964036284730103\n",
      "Epoch:  75 BCE:  0.2869798243045807 ROC train:  0.7961426155559521 ROC val:  0.7985706320345958\n",
      "Epoch:  76 BCE:  0.2460068166255951 ROC train:  0.7977275504567269 ROC val:  0.8004443915947196\n",
      "Epoch:  77 BCE:  0.30855411291122437 ROC train:  0.8006099116130733 ROC val:  0.7982694402634392\n",
      "Epoch:  78 BCE:  0.2699768841266632 ROC train:  0.7997774730219538 ROC val:  0.8003052239396484\n",
      "Epoch:  79 BCE:  0.2877114713191986 ROC train:  0.8030097592895383 ROC val:  0.8025953517000904\n",
      "Epoch:  80 BCE:  0.28218331933021545 ROC train:  0.8017749822393663 ROC val:  0.7986669656264391\n",
      "Epoch:  81 BCE:  0.21587735414505005 ROC train:  0.8033658856993291 ROC val:  0.8037319379593035\n",
      "Epoch:  82 BCE:  0.2930489182472229 ROC train:  0.8018148653032563 ROC val:  0.8048170491634594\n",
      "Epoch:  83 BCE:  0.29696574807167053 ROC train:  0.8043846127160801 ROC val:  0.8052980835088146\n",
      "Epoch:  84 BCE:  0.24610498547554016 ROC train:  0.8030240934265527 ROC val:  0.8055829691058138\n",
      "Epoch:  85 BCE:  0.2392958700656891 ROC train:  0.8052098518864446 ROC val:  0.8053509494235683\n",
      "Epoch:  86 BCE:  0.28445711731910706 ROC train:  0.8079899533688674 ROC val:  0.8066861018777528\n",
      "Epoch:  87 BCE:  0.19190366566181183 ROC train:  0.8077247596558764 ROC val:  0.8094269896041539\n",
      "Epoch:  88 BCE:  0.2379947155714035 ROC train:  0.8078686351456421 ROC val:  0.8073366202041375\n",
      "Epoch:  89 BCE:  0.2937949299812317 ROC train:  0.8080066842784956 ROC val:  0.8049071393406647\n",
      "Epoch:  90 BCE:  0.3149247467517853 ROC train:  0.808974124494527 ROC val:  0.8077940739583506\n",
      "Epoch:  91 BCE:  0.28360238671302795 ROC train:  0.8098663692296292 ROC val:  0.8056197650717873\n",
      "Epoch:  92 BCE:  0.31151631474494934 ROC train:  0.8075800602293951 ROC val:  0.8122849369395916\n",
      "Epoch:  93 BCE:  0.23341004550457 ROC train:  0.811946940321684 ROC val:  0.8104552278930015\n",
      "Epoch:  94 BCE:  0.23306411504745483 ROC train:  0.8106247563546678 ROC val:  0.8091609821678938\n",
      "Epoch:  95 BCE:  0.3491281270980835 ROC train:  0.8126566477697343 ROC val:  0.8098157790436309\n",
      "Epoch:  96 BCE:  0.2350524514913559 ROC train:  0.811846558554285 ROC val:  0.8124293611612303\n",
      "Epoch:  97 BCE:  0.29849138855934143 ROC train:  0.8126247538658817 ROC val:  0.8099122583445851\n",
      "Epoch:  98 BCE:  0.2750288248062134 ROC train:  0.8113654661130626 ROC val:  0.8113416691396373\n",
      "Epoch:  99 BCE:  0.29059532284736633 ROC train:  0.8127870930172282 ROC val:  0.8118803976698357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.BCELoss()\n",
    "learning_rate = 1e-6\n",
    "optimiser = torch.optim.RMSprop(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "#After 100 epochs LR will be on ~ 13% of the initial value\n",
    "lr_sched = lr_scheduler.ExponentialLR(optimiser, gamma=98e-2, last_epoch=-1)\n",
    "\n",
    "#####################\n",
    "# Train model\n",
    "#####################\n",
    "num_epochs = 100\n",
    "hist = np.zeros((2, num_epochs))\n",
    "\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in range(4):\n",
    "    predictions.append(np.array([]))\n",
    "\n",
    "roc_score = 0.0\n",
    "\n",
    "model.train()\n",
    "for t in tqdm(range(num_epochs)):\n",
    "    lr_sched.step()\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        x = batch[0].cuda()\n",
    "        y = batch[1].cuda()\n",
    "            \n",
    "        # Forward pass\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "\n",
    "        predictions[0] = np.append(predictions[0], y.cpu().numpy().reshape(-1), axis = 0)\n",
    "        predictions[1] = np.append(predictions[1], y_pred.detach().cpu().numpy().reshape(-1), axis = 0)    \n",
    "            \n",
    "        hist[0][t] = loss.item()\n",
    "\n",
    "         # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "         # Update parameters\n",
    "        optimiser.step()\n",
    "        \n",
    "    for test_batch in dataloader_test:\n",
    "        x_test = test_batch[0].cuda()\n",
    "        y_test = test_batch[1].cuda()\n",
    "        test_pred = model(x_test)\n",
    "        predictions[2] = np.append(predictions[2], y_test.cpu().numpy().reshape(-1), axis = 0)\n",
    "        predictions[3] = np.append(predictions[3], test_pred.detach().cpu().numpy().reshape(-1), axis = 0)\n",
    "\n",
    "#     for sublist in predictions[0]:\n",
    "#         for item in sublist:\n",
    "#             flat_list[0].append(item)\n",
    "#     for sublist in predictions[1]:\n",
    "#         for item in sublist:\n",
    "#             flat_list[1].append(item)\n",
    "#     for sublist in predictions[2]:\n",
    "#         for item in sublist:\n",
    "#             flat_list[2].append(item)\n",
    "#     for sublist in predictions[3]:\n",
    "#         for item in sublist:\n",
    "#             flat_list[3].append(item)\n",
    "    \n",
    "    roc_score = roc_auc_score(y_true = predictions[0], y_score = predictions[1])\n",
    "    roc_val = roc_auc_score(y_true = predictions[2], y_score = predictions[3])\n",
    "    print(\"Epoch: \", t, \"BCE: \", loss.item(), \"ROC train: \", roc_score, \"ROC val: \", roc_val)\n",
    "    hist[1][t] = roc_val\n",
    "    predictions = []\n",
    "    for i in range(4):\n",
    "        predictions.append(np.array([]))\n",
    "    #tqdm.write(\"Epoch: {}, BCE: {}, ROC train: {}, ROC val: {}\", t, loss_item(), roc_score, roc_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f633ce5e908>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
